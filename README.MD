API â†’ Azure Data Lake Pipeline

ğŸ§© Use Case

This project demonstrates how to ingest live data from a public API (such as weather, currency, COVID-19, or cryptocurrency data) into Azure Data Lake using a robust ETL pipeline. The pipeline ensures fault-tolerant, incremental ingestion with proper handling of complex JSON data.

ğŸ› ï¸ Tools

Azure Data Factory (ADF) â€“ REST connector for API ingestion

Azure Data Lake Storage (ADLS) â€“ Data storage and staging

Databricks â€“ Data transformation, JSON flattening, and incremental processing

âœ… Features Implemented

API Pagination â€“ Automatically retrieves all pages of data from the API.

Error Handling & Retries â€“ Implements retry logic for failed requests.

JSON Flattening â€“ Converts nested JSON responses into tabular format.

Incremental Ingestion â€“ Only ingests new or updated data to minimize storage and processing time.

ğŸ’¼ Skills Demonstrated

Building real-world ingestion pipelines

Integrating REST APIs with cloud storage

Implementing fault tolerance and retry mechanisms

Working with nested JSON data and incremental ETL
